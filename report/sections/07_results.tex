\section{Results}\label{sec:results}

%Här kan du till exempel presentera resultat av experiment, bevis, analys av data etc. Dina resultat måste beskrivas så tydligt att en läsare kan bedöma dem.  Du ska också förklara och analysera resultaten.

The results are divided into two relevant subsections where the first handles the input inference test and the second handles the 3D reconstruction using \aruco corners.
\subsection{OpenPose output inference results}%
\label{sub:res:op_inference}
Using \openpose{ } on a human subject that is located on the ground can provide mixed results depending on the relative rotation of the body of the camera.
How well does it perform when the subject is laying, such as in image North in figure~\ref{fig:camera_pos_lables}.
Moreover, how is the preference for each label in the \operpose data set against a human labeller?

During statistical tests with F/T-test, the degrees of freedom will tell how many data points are behind the results.
Thus can be shown in table~\ref{tab:results:degfreedom} the degrees of freedom for each label can be found.
While table~\ref{lab:results:human_vs_openpose} shows the degrees of freedom depends on the direction where the camera was located.

From the results in table~\ref{tab:results:human}, it can be observed that F-test is mostly rejected while the T-test is mainly reported as accept.
This then propose that the $H_0$ hypothesis is weekly rejected as it fails on $\sigma_H = \simgma_O$ but is passed on $\mu_H = \mu_O$.
Therefore the $H_1$ hypothesis is accepted, and that shows that \openpose{ } do not have the same accuracy as a human label setter.

The directional error shown as a box plot in figure~\ref{fig:results:dirbox} shows how South provides the smaller error compared to the quadrants.
But is that significant enough to be a problem?
According the p-value in table~\ref{lab:results:dir_stats} that is not the case as the p-value is higher then 0.05 in all cases except East - North.

\begin{figure}
\begin{center}
    \includegraphics[scale=0.4]{../results/ftest_againstself_boxplot.pdf}
    \includegraphics[scale=0.4]{../results/direction_hist_plot.pdf}
\end{center}
\caption{In this figure the individual error for each direction is plotted. Observe that the error from South has a smaller footprint then the rest of the boxes. Could this be a part of the problem \openpose faces when the subject is laying on the ground?. However by inspecting the histograms on the left the data is clearly not normally distributed. }
\label{fig:results:dirbox}
\end{figure}



%----------------------------
\begin{table}[htb]
    \begin{center}
    \begin{minipage}{0.4\textwidth}
        \begin{center}
            \input{../results/error_degdf_df.latex}
        \end{center}
        \caption[Degrees of freedom human vs openpose]{The degrees of freedom for human and \openpose is due to the quite limited dataset not in most cases not statistically viable but perhaps it cold work as a marker.}
        \label{tab:results:degfreedom}
    \end{minipage}
    \begin{minipage}{0.4\textwidth}
        \begin{center}
            \input{../results/direction_degdf_df.latex}
        \end{center}
        \caption[Directional degrees of freedom]{The directional degrees of freedom is a bit better because it do not care about the labels, just the total error for that direction. }
        \label{tab:results:dirdegfreedom}
    \end{minipage}
    \end{center}
\end{table}
%----------------------------
\begin{table}[htb]
    \begin{center}
        % \input{../results/ftest_pos_df.latex}
        \input{../results/direction_error_df.latex}
    \end{center}
    \caption{Testing \openpose directional invariance against its own data provides the following results for the F-test based on ANOVA\@. The p-values indicate that the $\text{h}_0$ holds for the most parts except in the case East to North, where $p=0.018$ is $p<\alpha$ for a $\alpha=0.05$\@. However the large q-value points to that this low p-value is a false positive or as shown in~\ref{fig:results:dirbox} a non normal distributed data.}
    \label{tab:results:dir_stats}
\end{table}
%----------------------------
\begin{table}[htb]
    \begin{center}
        \input{../results/error_df.latex}
    \end{center}
    \caption[Results in image domain]{The error results for the human vs \openpose in the image domain. Observe the large variance in the fourth column that suggests that \openpose have a problem finding the correct solution for that label.
It can also be observed that half of the data is missing in the \openpose column. Thus indicating again that it could not find a solution to that label.}
    \label{lab:results:human_vs_openpose}
\end{table}
%----------------------------
\begin{table}[htb]
    \begin{center}
        \input{../results/ftest_pds.latex}
    \end{center}
    \caption[F/T-test results for labels]{This table shows the result from each label using F/T-tests. The majority of the results suggests that the $H_0$ hypotheses is rejected.}
    \label{tab:results:human}
\end{table}


\subsection{3D reconstruction using Aruco}%
Reconstruction of the sparse 3D map done by using \aruco{'s} in a pose quiver without solving the bundle adjustment problem was attempted to be solved.
The method derived relied on the Dijkstra algorithm, and cumulative transfer poses from corner to corner to camera.
And then, by solving the epee polar geometry, a point cloud is supposed to be generated.
% However, due to the algorithms' cumulative output, the positions of the camera are never reaching a satisfiable position.
However, due to a bug never being resolved, the exact camera position was not resolvable.
In the figure~\ref{fig:correct_pose} an approximated result for where the camera and \aruco{ } markers are located can be found.
Those images were produced by implementing the following transformation into a PyGame\footnote{PyGame a python library for game development \url{www.pygame.org}} script.


\begin{align}
    p_{[2+1xN]} & = \lambda \cdot Y^{-1}_{2x2}\cdot D^{T}_{Nx2} \label{eq:pygame:pixelstack}\\
    P_{[2+1xN]} & = T_{z}(0,dx,dy) \cdot T_{z}(\alpha,0,0) \cdot p_{[2+1xN]} \label{eq:pygame:tf}
\end{align}
The input equation~\ref{eq:pygame:pixelstack} takes the input $D$h and does a scaling and y-axis invert.
The results are stored in a homogeneous matrix marked as $p_{[2+1xN]}$.
Thus the following list covers the explanation of each term in~\ref{eq:pygame:pixelstack}:
% \vspace{5mm}


\begin{align*}
    D_{Nx2}   &=       \text{is the 2D projected data with $N$ features in $[x,y]$ configuration.}\\
    Y^{-1}    &=       \text{Is an 2 by 2 matrix for Y axis invert because of how 2D graphics works.}\\
    \lambda   &=      \text{Is a scaling factor to scale the data to fit the image}\\
    p_{[2+1xN]} &=      \text{Is the results formatted in a homogeneous coordinates.}
\end{align*}

% \vspace{5mm}
And in equation~\ref{eq:pygame:tf} the results from~\ref{eq:pygame:pixelstack} is used.
To cause with two transfer matrices to cause an homogeneous transformation of the data.
Thus displacing it on the image.
% \vspace{5mm}
\begin{align*}
    p(x,y,1) &=  \text{Is a homogeneous coordinate matrix of $3xN$ where N is number of features.}\\
    T_{z}    &=  \text{Are a homogeneous transformations with the arguments:}\\
             & \alpha  = \text{Rotation around Z axis.}\\
             & dx =      \text{Movement in X axis.}\\
             & dy =      \text{Movement in Y axis.}
\end{align*}

% \vspace{5mm}
The first transfer from right to left is used to rotate the data around the z-axis before transferring it out to the location of choice in the last transfer matrix.



\begin{figure}
\begin{center}
    \includegraphics[scale=0.25]{../results/correct_pos/corners.png}
    \hspace{1cm}
    \includegraphics[scale=0.25]{../results/correct_pos/camera.png}
\end{center}
\caption[Approx locations of markers and cameras]{This figure is a outline trace of an approximation for how the subject is laying on the ground.
    That is due to that no such image exists; therefore its an approximation.
Cameras and corner position is deduced from several images, therefore an approximation.
    On the \textbf{left} the relative position of the \aruco{ } corners are displayed in in relation to the origin corner.
    To the \textbf{right} the relative position of the cameras is displayed in relation to the \aruco{ } origin corner.
Moreover, as can be observed, the corners have a good match, while the camera locations do not.}
\label{fig:correct_pose}
\end{figure}


